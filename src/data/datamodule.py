from data.dataset import MixDataset
from typing import List
from torch.utils.data import DataLoader
import pytorch_lightning as pl


class HDF5DataModule(pl.LightningDataModule):

    def __init__(self,
                 n_channels: int,
                 batch_size: int,
                 prep_files: dict,
                 stft_length_samples: int,
                 stft_shift_samples: int,
                 snr_range: List[int],
                 meta_frame_length: int,
                 n_workers: int,
                 dry_target: bool = True,
                 target_dir = 0,
                 noise_snr: List[int] = None,
                 fs: int = 16000
                 ):
        """
        Init the data module for the simulated CHiME3 dataset.

        :param n_channels: number of channels in the microphone array (must be smaller or equal to prep_files)
        :param batch_size: the batch size
        :param prep_files: a dictionary specifying the HDF5 data file and meta data file generated by the preprocessing.py files. Keys are 'data', 'meta', 'train_data', 'train_meta', 'val_data', 'val_meta', 'test_data' and 'test_meta'. The 'data' and 'meta' keys serve as default if other keys are not specified.
        :param meta_frame_length: the metaframe length (e.g. randomly cut 1 second of data -> 16000 samples per metaframe)
        :param dry_target: use dry clean signal if True else use reverberant signal
        :param fs: sampling rate (default 16 kHz)
        :param snr_range: the list of possible SNRs (sampled from for training example generation)
        :param noise_snr: a list of SNRs for additive white noise (no noise if None)
        :param n_workers: number of workers for data loading
        """
        super().__init__()

        self.batch_size = batch_size
        self.fs = fs
        self.meta_frame_len = meta_frame_length
        self.snr_range = snr_range

        self.n_channels = n_channels
        self.stft_len = stft_length_samples
        self.stft_shift = stft_shift_samples

        self.target_dir = target_dir
        self.noise_snr = noise_snr

        self.n_workers = n_workers

        self.train_dataset = MixDataset(stage='train', 
                                        prep_files=prep_files, 
                                        n_channels=self.n_channels, 
                                        meta_frame_length=self.meta_frame_len,
                                        disable_random=False, 
                                        snr_range = self.snr_range,
                                        dry_target=dry_target,
                                        target_dir= self.target_dir, 
                                        noise_snr=self.noise_snr)
        self.val_dataset = MixDataset(stage='val', 
                                        prep_files=prep_files, 
                                        n_channels=self.n_channels, 
                                        meta_frame_length=self.meta_frame_len,
                                        disable_random=True, 
                                        snr_range = self.snr_range,
                                        dry_target=dry_target,
                                        target_dir= self.target_dir, 
                                        noise_snr=self.noise_snr)
        

    def train_dataloader(self):
        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=self.n_workers, shuffle=True)

    def val_dataloader(self):
        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=self.n_workers, shuffle=False)